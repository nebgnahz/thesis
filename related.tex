\documentclass[thesis.tex]{subfiles}

\begin{document}

\chapter{Related Work}
\label{cha:related-work}

In this thesis, we propose to use systematic adaptation and quantitative
profiling to develop resilient swarm applications. We have summarized the trend
of the emerging swarm in \autoref{cha:background}. In this chapter, we focus on
research projects and industrial effort related to our techniques. We group
related work into four categories: approximation and adaptation, taming
constrained resources, efficient profiling, and other software systems.

\section{Approximation and Adaptation}
\label{sec:appr-adapt}

The idea of using approximation to trade application performance (such as
accuracy) for system performance (such as response times) has been explored in a
number of contexts.  Hellerstein et al.~\cite{hellerstein1997online} describes
an online aggregation interface that provides approximate answers which are
constantly refined as during execution. BlinkDB~\cite{agarwal2013blinkdb}
enables interactive queries over massive data by allowing users to trade-off
query accuracy. MEANTIME~\cite{farrell2016meantime} uses approximate computing
to achieve both hard real-time guarantees and energy efficiency. In these
applications, partial or approximate results are more useful than late
results. In addition to response times, some systems optimize for energy
consumption. For example, EnerJ~\cite{sampson2011enerj} uses programming
language support to isolate parts of the program that must be precise from those
that can be approximated to trade accuracy for energy. In this thesis, our
research is along the same line: many swarm applications can reduce data
fidelity or algorithm complexity, trading application accuracy for low latency.

% In 1990s, Odyssey~\cite{noble1995programming} has proposed
% \emph{application-aware adaptation} for mobile computing and explored data
% fidelity.
% sampling~\cite{garofalakis2001approximate}, sketches~\cite{cormode2011sketch}

For our work on network resource adaptation, the closest system to \awstream{}
is JetStream~\cite{rabkin2014aggregation}. JetStream is the first to use
degradation to reduce latency for wide-area streaming analytics. However,
JetStream relies on manual policies that are sub-optimal. \awstream{} instead
proposes a novel API design to specify degradation and supports automatic
profiling to search for Pareto-optimal configurations. \awstream{} also makes
the following additional contributions: $(1)$~online profiling to address model
drift; $(2)$~an improved runtime system achieving sub-second latency;
$(3)$~support for different resource allocation policies for multiple
applications.

Adaptive video streaming is another domain in which network resource adaptation
has been extensively studied. Multimedia streaming protocols
(e.g.,~RTP~\cite{schulzrinne2006rtp}) aim to be fast instead of reliable. While
they can achieve low latency, their accuracy can be poor under congestion.
Recent work has moved towards HTTP-based protocols and focused on designing
adaptation strategy to improve QoE, as in research~\cite{mao2017neural,
  sun2016cs2p, yin2015control} and industry (HLS~\cite{pantos2016http},
DASH~\cite{michalos2012dynamic, sodagar2011mpeg}). These adaptation strategies
are often pull-based: client keeps checking the index file for changes. And
clients have to wait for the next chunk (typically 2-10 seconds). In addition,
as we have shown in \autoref{sec:runtime-adaptation}, their adaptation is a poor
match for analytics that rely on image details (e.g.,~6\% accuracy for PD). In
contrast, we propose to \emph{learn} the adaptation strategy for each
application (also not limited to video analytics).

The use of adaptation is not to provide hard Quality of Service (QoS)
guarantees. In \awstream{}, we maximize application accuracy and minimize
bandwidth requirement: a multi-dimensional optimization. And when the available
bandwidth meets application requirement, we achieve low latency streaming.  In
addition, \awstream{} adopts an end-host application-layer approach ready today
for WAN, while a large portion of network QoS work~\cite{ferrari1990scheme,
  shenker1994integrated, shenker1995fundamental} in the 1990s focuses on
network-layer solutions that are not widely deployable. For other
application-layer approaches~\cite{vandalore2001survey}, we can apply our
systematic adaptation approach by incorporating their techniques, such as
encoding the number of layers as a knob to realize the layered approach in
Rejaie et al.~\cite{rejaie2000layered}.

% \para{SLO-awareness.} MittOS~\cite{hao2017mittos} advocates the return of
% \texttt{EBUSY} for IO operations to bound IO requests.

%% TCP, for example, adapts to available bandwidth by controlling the congestion
%% window with AIMD~\cite{jacobson1988congestion}.  Previous work has proposed
%% modifications to TCP for specific contexts. For streaming over TCP, Goel et
%% al.~\cite{goel2008low} proposes adaptive buffer-size tuning. For the cloud,
%% Adaptive TCP~\cite{wu2013adaptive} proposes to modify the congestion control
%% behavior based on flow size for the cloud.

% \para{Scheduling and allocation:} Resource allocations in the cloud is how to
% efficiently allocate tasks.  In the wide area, we face fundamental limit that
% therefore degradation is more effective. For multiple tasks, Existing
% allocations are for resources without considering application trade-offs.
% MediaNet~\cite{hicks2003user} uses both local and online global resource
% scheduling to improve user performance and network utilization, and adapts
% without requiring underlying support for resource
% reservations. VideoStorm~\cite{zhang2017live} primarily focuses on cluster
% resource allocation among video queries. For wide-area, the resource
% allocation is limited. We do not control the capacity; but still we can
% allocate the available bandwidth.

% Performance modeling: CherryPick~\cite{alipourfard2017cherrypick},
% Ernest~\cite{venkataraman2016ernest}.
%% Dolly~\cite{ananthanarayanan2013effective}.
%% \para{Program Synthesis:}

\section{Taming Constrained Resources}
\label{sec:taming-constr-reso}

In this thesis, we have addressed different types of constrained resources:
$(i)$ network resources, i.e., scarce and variable WAN bandwidth; $(ii)$ compute
resources, i.e., heterogeneous swarm devices. We also have studied the impact of
variability, both network delays and service overload, in heavy-computation
tasks. We discuss related work that addresses constrained resources.

\para{Scarce and Variable WAN Bandwidth}. Geo-distributed systems need to cope
with high latency and limited bandwidth in the wide area. While some systems
assume the network can prioritize a small amount of critical data under
congestion~\cite{cho2012surviving}, most systems reduce data sizes in the first
place to avoid congestion (e.g.,~LBFS~\cite{muthitacharoen2001low}). Over the
past few years, we have seen a plethora of geo-distributed analytical frameworks
that incorporate heterogeneous wide-area bandwidth into application execution to
minimize data movement, as follows.

\begin{itemize}[topsep=5pt, leftmargin=20pt, itemsep=2pt]

\item Pixida~\cite{kloudas2015pixida} minimizes data movement across inter-DC
  links by solving a traffic minimization problem in data analytics.

\item Iridium~\cite{pu2015low} optimizes data and task placement jointly to
  achieve an overall low latency goal.

\item Geode~\cite{vulimiri2015global} develops input data movement and join
  algorithm selection strategies to minimize WAN bandwidth usage.

\item WANalytics~\cite{vulimiri2015wananlytics} pushes computation to edge data
  centers, automatically optimizing workflow execution plans and replicating
  data only necessary.

\item Clarinet~\cite{viswanathan2016clarinet} incorporates bandwidth information
  into its query optimizer to reduce data transfer.

  % \item SWAG~\cite{hung2015scheduling} coordinates compute task scheduling
  %   across DCs.

\end{itemize}

While effective in speeding up analytics, these systems focus on batch tasks
such as MapReduce jobs or SQL queries. Such tasks are executed infrequently and
without real time constrain. In contrast, many swarm applications process
streams of sensor data continuously in real time.

\para{Offloading to Augment Mobile Devices.} To improve application performance
and extend battery life for mobile devices, research has explored offloading
heavy-computation tasks to remote servers. Satyanarayanan used the word ``cyber
foraging'' to refer to dynamically augmenting the computing resources of a
wireless mobile computer by exploiting wired hardware
infrastructure~\cite{satyanarayanan2001pervasive, satyanarayanan2015brief}.

Offloading receives much attention, especially after mobile computing took off
since 2010. MAUI~\cite{cuervo2010maui} supports fine-grained code offloading by
exploiting the managed code environment. Developers annotate candidate methods
to offload as \texttt{Remoteable}. MAUI then uses an optimization engine to
decide which method to offload dynamically. CloneCloud~\cite{chun2011clonecloud}
is a step further that partitions applications without developer annotation. In
both MAUI and CloneCloud, the code either runs on a mobile device or on a
server. Tango~\cite{gordon2015accelerating} explores the use of replication: the
application executes on both the mobile device and the server. It allows either
mobile devices or the server to lead the execution and uses flip-flop
replication to allow leadership to float between replicas.

\para{Redundancy to Tackle Latency Variability.} The idea of initiating
redundant operations across diverse resources and using the first result which
completes has been explored in many contexts. For mobile computing, we have
mentioned Tango~\cite{gordon2015accelerating} that replicates execution on both
mobile devices and the server. Cloud computing often has straggler tasks that
delay the job completion. Instead of waiting and trying to predict stragglers,
Dolly~\cite{ananthanarayanan2013effective} launches multiple clones of every
task of a job and uses the result that finishes first. For cloud-based web
services at Google, Dean et al.~\cite{dean2013tail} describe similar redundancy
techniques that address the tail performance at scale, such as hedged requests
and tied requests. In wide-area network, Vulimiri et al.~\cite{vulimiri2013low}
show that redundancy reduces latency for a number of services, such as DNS
queries, database queries, and packet forwarding.

For offloading, mobile devices and servers run the same algorithm (with the same
configurations): either by partition or replication.  For redundancy, an array
of worker machines also run the same task. This fails to accommodate the
heterogeneous swarm environment. The proposed adaptation in this thesis
explicitly models performance across different devices. The learned profile can
be used to assist offloading and replication.

\section{Efficient Profiling}
\label{sec:prof-with-effic}

Prior work also use profiling to guide adaptation. For SQL queries, BlinkDB
creates error-latency profiles to guide sample selection to meet the query's
response time or accuracy requirements. For large scale video processing,
VideoStorm~\cite{zhang2017live} considers resource-quality profile and lag
tolerance, and trades off between them.

For an efficient profiling, VideoStorm uses coordinate search---or greedy search
in their terms. This thesis makes two major contributions: $(1)$~our proposed BO
outperforms coordinate search approach used in VideoStorm; $(2)$~we handle
heterogeneous devices while VideoStorm considered homogeneous machines in
clusters.

BO, a technique for global optimization of black-box functions, has been applied
to solve a wide range of problems. Snoek et al.~\cite{snoek2012practical}
demonstrates its effectiveness in tuning parameters and modeling hyperparameters
for ML tasks. CherryPick~\cite{alipourfard2017cherrypick} uses BO to minimize
cloud usage cost by searching for the best cloud configurations: the number of
VMs, the number of cores, CPU speed per core, average RAM per core, disk speed,
etc. FLASH~\cite{zhang2016flash} proposes a two-layer Bayesian optimization
framework to tune hyperparameters in data analytic
pipelines. BOAT~\cite{dalibard2017boat} extends BO with by leveraging contextual
information from developers and demonstrates the use in garbage collection and
neural network. Google uses BO to optimize the recipe for chocolate chip
cookies~\cite{solnik2017bayesian}. These use cases are single-objective
optimization: minimizing time, cost or maximizing utility, tastiness. In this
thesis, we face a multi-dimensional optimization between application accuracy
and processing times.

\section{Related Systems}
\label{sec:related-systems}

\awstream{} has large debt to existing stream processing systems both for
programming abstractions and handling back pressure. Early streaming
databases~\cite{abadi2005design, chandrasekaran2003telegraphcq} have explored
the use of dataflow models with specialized operators for stream
processing. Recent research projects and open-source
systems~\cite{akidau2013millwheel, toshniwal2014storm, sanjeev2015twitter,
  zaharia2013discretized, carbone2015apache} primarily focus on fault-tolerance
in the context of a single cluster. When facing back pressure,
Storm~\cite{toshniwal2014storm}, Spark Streaming~\cite{zaharia2013discretized}
and Heron~\cite{sanjeev2015twitter} throttle data ingestion; Apache
Flink~\cite{carbone2015apache} uses edge-by-edge back-pressure techniques
similar to TCP flow control; Faucet~\cite{lattuada2016faucet} leaves the flow
control logic up to developers. In contrast, \awstream{} targets at the wide
area and explicitly explores the trade-off between data fidelity and freshness.

Our compute resource adaptation work focuses on heavy-computation tasks, such as
machine learning inference. Systems such as Clipper~\cite{crankshaw2017clipper}
and TensorFlow Serving~\cite{tensorflow2017serving} are state-of-the-art in
serving ML models and performing inference. They only focus on system-level
techniques such as batching and hardware acceleration to improve throughput. And
our adaptation, especially the profile, can be used by these systems as
application-aware improvements.

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
