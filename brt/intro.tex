\section{Introduction}
\label{sec:introduction}

An increasing number of swarm applications rely on heavy computation tasks, such
machine learning (ML). For example, Microsoft SeeingAI~\cite{seeingai} is a
talking camera for the blind and low vision community that describes nearby
people, text, objects. Wearable cognitive assistant~\cite{ha2014towards} uses
wearable devices, such as Google Glass, for deep cognitive assistance, e.g.,
offering hints for social interaction via real-time scene analysis.

While there is a staggering collection of research focusing on model
\textit{training} for these tasks, for swarm applications, they perform the
\textit{inference} step of machine learning: use trained models to make a
prediction given an input, such as visual~\cite{googlelens, ha2014towards,
  seeingai}, audio~\cite{alexa, applesiri, cortana}, and sensory
information~\cite{laput2017synthetic, lu2010jigsaw}. Inference has received
relatively little attention, especially on edge and end devices.

One important challenge for inference is to bound the response time, which
affects user experience significantly in these human-in-the-loop
systems. Previous usability studies~\cite{nielsen1994usability,
  schneiderman1998designing} have measured the effect of different end-to-end
latency:

\begin{itemize}[noitemsep, topsep=5pt]
\item 100 ms gives the feeling of instantaneous response.
\item 1 second keeps the user's flow of thought but users can sense the delay.
\item A few seconds' delay creates an unpleasant user experience.
\end{itemize}

Achieving bounded response times (BRT) on swarm devices has remained
challenging. Swarm devices are extremely heterogeneous and many are resource
constrained (see \autoref{tab:embedded}). Many heavy computations are simply
beyond the capabilities of cheap end devices. State-of-the-art computer vision
algorithms, such as object detection, easily take several seconds on modern
smartphones~\cite{chen2015glimpse}.

To compensate resource-constrained devices, prior work explores
offloading~\cite{chun2011clonecloud,cuervo2010maui} to the cloud or the edge
that hosts machine learning models and performs heavy computation. Despite their
powerful computing capability and more available resources, e.g., special
hardware like GPU and TPU~\cite{jouppi2017datacenter}, these platforms suffer
from increased network latency and service overload (\autoref{fig:edge}). As a
result, they are often unable to meet latency goals consistently.

Another technique for low latency is to use redundant requests, effective for
variability in network delays~\cite{gordon2015accelerating, vulimiri2013low} and
slow servers (known as straggler mitigation in the cloud~\cite{dean2013tail,
  ananthanarayanan2013effective}). Redundancy refers to initiating the multiple
requests across a diverse set of resources and using the first result that
completes. Existing work treats these resources equally and executes the same
task on each platform, a poor match for the heterogeneous swarm space.


%% //////////////////////////

Our insight is that for a particular inference task, multiple algorithms or
tunable parameters for one algorithm exist that requires different processing
times and result in different accuracy. To accommodate the resource-constrained
devices, one can use an algorithm or a parameter that suits the particular
platform.

\begin{figure}
  \centering
  \includegraphics[width=0.8\columnwidth]{figures/dr.pdf}
  \caption{Illustration of offloading, redundant requests and differential
    redundancy.}
  \label{fig:dr}
\end{figure}

We propose to adapt the computation (as in \autoref{fig:dr}) to each individual
platform by leveraging the choices of algorithms and parameters for a particular
inference task. With a precise performance model that captures the tradeoff
space between application accuracy and processing times, we can achieve such
adaptation and then bounded response times. This adaptation technique is
orthogonal to existing system solutions, such as scale-out, scale-up, caching,
or batching.

On the client side, it uses the configuration best suited for its particular
hardware capability.

On the server side, each request is annotated with a service-level objective
(SLO) that describes the deadline and accuracy demand. Upon receiving a request,
the server runs a real-time scheduler to meet all SLOs. $(i)$ \brt{} can adjust
each request's serving time and quality based on the profile; $(ii)$ \brt{} is
allowed to reject a request because of the redundancy mentioned above.

Because \brt{} is application-aware, it requires knowledge from the application
developer. And therefore a well-designed abstraction.

Efficient profiling to find the performance model. In order to explore the
trade-off space, we take a data-driven approach to learn the relationship
between inference accuracy and processing times for each algorithm and different
parameters for individual platforms. We call this process ``profiling'' and its
goal is to find Pareto-optimal configurations (algorithm and parameter) as the
performance model.

Profiling with an exhaustive search may not be feasible for some algorithms due
to their large parameter space. In these cases, we use a statistical method,
Bayesian Optimization (BO), to model the relationship as a black-box function
and only searches for near-optimal configurations.

Profiling for all hardware platforms is not feasible due to the large space and
availability of the hardware at development time. We observe that the profile
can be easily transferred across platforms with linear transformation.

We make the following contributions in this chapter with \brt{}:

\begin{itemize}

\item The proposed Bayesian Optimization (BO) for profiling, significantly
  outperforms previous approaches with random search or coordinate/greedy
  approach.
\item We empirically validate the profile transfer to address heterogeneous
  capabilities across swarm platforms.

\end{itemize}

% \begin{figure*}
%   \centering
%   \includegraphics[width=\textwidth]{figures/platforms}
%   \label{fig:options}
%   \caption{Platforms}
% \end{figure*}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../compute"
%%% End:
